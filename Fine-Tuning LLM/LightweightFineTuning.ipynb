{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: Lora\n",
    "* Model: Bert\n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71fc61b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/student/.local/lib/python3.10/site-packages (from transformers[torch]) (24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/student/.local/lib/python3.10/site-packages (from transformers[torch]) (4.66.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\n",
      "Requirement already satisfied: requests in /home/student/.local/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/.local/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/student/.local/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/student/.local/lib/python3.10/site-packages (from transformers[torch]) (0.21.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.25.0)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.0.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/student/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.10.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/student/.local/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers[torch]) (2.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/student/.local/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.36.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.1)\n",
      "Requirement already satisfied: pyyaml in /home/student/.local/lib/python3.10/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/student/.local/lib/python3.10/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\n",
      "Requirement already satisfied: tqdm in /home/student/.local/lib/python3.10/site-packages (from peft) (4.66.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/student/.local/lib/python3.10/site-packages (from peft) (24.0)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/student/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (4.10.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/student/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: huggingface-hub in /home/student/.local/lib/python3.10/site-packages (from accelerate->peft) (0.21.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/student/.local/lib/python3.10/site-packages (from transformers->peft) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/student/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate->peft) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/student/.local/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers->peft) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers->peft) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers->peft) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers->peft) (3.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: evaluate in /home/student/.local/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/student/.local/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/student/.local/lib/python3.10/site-packages (from evaluate) (4.66.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/student/.local/lib/python3.10/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/student/.local/lib/python3.10/site-packages (from evaluate) (2023.10.0)\n",
      "Requirement already satisfied: pandas in /home/student/.local/lib/python3.10/site-packages (from evaluate) (2.2.1)\n",
      "Requirement already satisfied: multiprocess in /home/student/.local/lib/python3.10/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: responses<0.19 in /home/student/.local/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/student/.local/lib/python3.10/site-packages (from evaluate) (0.21.4)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/student/.local/lib/python3.10/site-packages (from evaluate) (2.15.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: packaging in /home/student/.local/lib/python3.10/site-packages (from evaluate) (24.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: dill in /home/student/.local/lib/python3.10/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/.local/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.local/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/student/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install -q \"datasets==2.15.0\"\n",
    "! pip install 'transformers[torch]'\n",
    "! pip install peft\n",
    "! pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e4cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"dair-ai/emotion\", split='train[0:1400]')\n",
    "dataset_split = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b29f6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 1400\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1120\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 280\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c32450a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i dont feel inhibited and i can work out my problems', 'label': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_split['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4504f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"sadness\",\n",
    "    1: \"joy\",\n",
    "    2: \"love\",\n",
    "    3: \"anger\",\n",
    "    4: \"fear\",\n",
    "    5: \"surprise\"\n",
    "}\n",
    "\n",
    "\n",
    "label2id = {label: id for id, label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "891c1510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457a1819131f4274a44c375b8c882b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cf221f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForSequenceClassification(\n",
       "  (transformer): T5Model(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): T5ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd3a8195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5b3cbafbb84aa882862ab1bb267db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddef4dc0af344fe875ab164fb16c0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_datasets = dataset_split['train'].map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset =  dataset_split['test'].map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb8bd50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i ask her what shed like to do and she just says she doesnt mind so i am always making suggestions and just feel like im having to try every day to keep her entertained',\n",
       " 'label': 1,\n",
       " 'input_ids': [3,\n",
       "  23,\n",
       "  987,\n",
       "  160,\n",
       "  125,\n",
       "  12478,\n",
       "  114,\n",
       "  12,\n",
       "  103,\n",
       "  11,\n",
       "  255,\n",
       "  131,\n",
       "  845,\n",
       "  255,\n",
       "  744,\n",
       "  17,\n",
       "  809,\n",
       "  78,\n",
       "  3,\n",
       "  23,\n",
       "  183,\n",
       "  373,\n",
       "  492,\n",
       "  5782,\n",
       "  11,\n",
       "  131,\n",
       "  473,\n",
       "  114,\n",
       "  256,\n",
       "  578,\n",
       "  12,\n",
       "  653,\n",
       "  334,\n",
       "  239,\n",
       "  12,\n",
       "  453,\n",
       "  160,\n",
       "  24943,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_datasets[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7db5a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "823f508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,        \n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,                  \n",
    "    args=training_args,                \n",
    "    train_dataset=tokenized_train_datasets,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ca79a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8491137027740479, 'eval_accuracy': 0.15357142857142858, 'eval_runtime': 9.1468, 'eval_samples_per_second': 30.612, 'eval_steps_per_second': 0.984}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5775fadf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(task_type=\"SEQ_CLS\",\n",
    "                        r=4,\n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=0.01,\n",
    "                        target_modules = ['query'])\n",
    "lora_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 82,956 || all params: 109,565,196 || trainable%: 0.07571382430603237\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7eded687",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_trainer = Trainer(\n",
    "    model=lora_model,                  \n",
    "    args=training_args,                \n",
    "    train_dataset=tokenized_train_datasets,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c26fb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/175 04:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.754861</td>\n",
       "      <td>0.153571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.702869</td>\n",
       "      <td>0.228571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.741100</td>\n",
       "      <td>1.675855</td>\n",
       "      <td>0.271429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.741100</td>\n",
       "      <td>1.663428</td>\n",
       "      <td>0.296429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.741100</td>\n",
       "      <td>1.660052</td>\n",
       "      <td>0.296429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=175, training_loss=1.7068409075055804, metrics={'train_runtime': 242.4069, 'train_samples_per_second': 23.102, 'train_steps_per_second': 0.722, 'total_flos': 1474822560153600.0, 'train_loss': 1.7068409075055804, 'epoch': 5.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"bert-lora\", save_adapter=True, save_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f2bd518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc3a8147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6600520610809326, 'eval_accuracy': 0.29642857142857143, 'eval_runtime': 8.9605, 'eval_samples_per_second': 31.248, 'eval_steps_per_second': 1.004, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "lora_results = lora_trainer.evaluate()\n",
    "print(lora_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b0cd6e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i just wish okay so i was thinking about it earlier today and heres the thing being all cooped up amp restless has made me feel so needy'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_dataset[100]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ed9d2d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_dataset[100]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3079df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = lora_trainer.predict(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'label']]\n",
    "df[\"predicted_label\"] = np.argmax(prediction[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6a542639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel i am completely dissatisfied with the w...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i hope to feel a bit more creative again soon ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i become overwhelmed and feel defeated</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel your innocent love</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i hear about a teenaged girl devastated by the...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  predicted_label\n",
       "0  i feel i am completely dissatisfied with the w...      3                0\n",
       "1  i hope to feel a bit more creative again soon ...      1                1\n",
       "2             i become overwhelmed and feel defeated      0                0\n",
       "3                          i feel your innocent love      1                1\n",
       "4  i hear about a teenaged girl devastated by the...      0                1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"bert-lora\", num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6f007603",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i just wish okay so i was thinking about it earlier today and heres the thing being all cooped up amp restless has made me feel so needy\"\n",
    "infer_text = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    infer_output = lora_model(**infer_text)\n",
    "    logits = infer_output.logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e060d63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: sadness\n"
     ]
    }
   ],
   "source": [
    "probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "predicted_class = torch.argmax(probs, dim=1)\n",
    "label = id2label[predicted_class.numpy()[0]]\n",
    "print(\"Predicted class:\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a1d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
